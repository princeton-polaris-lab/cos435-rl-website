<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>COS 435 / ECE 433: Reinforcement Learning - Spring 2026</title>
    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">
  </head>
  
  <body>
    <div class="container">
      <h1 style="margin-bottom:0.5em; color: #EE7F2D; text-align: center;">
        <a href="index.html" style="text-decoration: none; color: inherit;">
          <strong style="font-weight:700">COS 435 / ECE 433:</strong> Reinforcement Learning
        </a>
      </h1>
      
      <!-- Princeton Polaris Lab logo under class title -->
      <div style="margin: 1rem 0 2rem 0; text-align: center;">
        <a href="/#/" style="display: inline-flex; align-items: center; gap: 0.75rem; text-decoration: none; opacity: 0.8; transition: opacity 0.2s;" onmouseover="this.style.opacity='1'" onmouseout="this.style.opacity='0.8'">
          <img src="../Geometric Diamond Logo Design.png" alt="Princeton Polaris Lab" style="height: 40px; width: auto;">
          <span style="font-size: 1.2rem; font-weight: 600; color: #333; letter-spacing: -0.01em;">Princeton Polaris Lab</span>
        </a>
      </div>
      
      <div class="header-row">
        <div class="course-info">
          <h5 style="color: #EE7F2D;">PRINCETON UNIVERSITY, SPRING 2026</h5>
          <h5><strong>Location:</strong> Friend Center 101</h5>
          <h5><strong>Time:</strong> Friday 1:20pm-4:10pm</h5>
        </div>
        <div class="nav-links">
          <a href="#" style="font-weight:600; color: #000;">Home</a> &nbsp;&nbsp;&nbsp;| 
          <a href="#course-description" style="color: #000;">Course Description</a> &nbsp;&nbsp;&nbsp;|
          <a href="#schedule" style="color: #000;">Schedule</a> &nbsp;&nbsp;&nbsp;|
          <a href="#assignments" style="color: #000;">Assignments</a> &nbsp;&nbsp;&nbsp;|
          <a href="#project" style="color: #000;">Project</a> &nbsp;&nbsp;&nbsp;|
          <a href="#faq" style="color: #000;">FAQ</a>
        </div>
      </div>

      <div class="namecard-container" style="align-items: flex-start;">
        <div class="namecard">
          <h6 class="namecard-header">Instructor</h6>
          <div class="namecard-content">
            <div class="profile-photo">
              <a href="https://www.peterhenderson.co/" target="_blank">
                <img src="../ai-law-2025/imgs/Peter.jpg" alt="Instructor">
              </a>
            </div>
              <div class="profile-details">
                <p><a href="https://www.peterhenderson.co/" target="_blank">Prof. Peter Henderson</a></p>
                <p>Assistant Professor in CS/SPIA</p>
                <p>Office Hours: By appointment</p>
              </div>
          </div>
        </div>
        <div class="namecard">
            <h6 class="namecard-header">Teaching Assistants</h6>
            <div class="namecard-content" style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5em;">
              <div class="profile-details">
                <p><strong>Zeyu Shen</strong></p>
                <p>Office Hours: Fri 9-10am</p>
                <p>Sherred Hall 3rd Floor</p>
              </div>
              <div class="profile-details">
                <p><strong>Kincaid MacDonald</strong></p>
                <p>Office Hours: Wed 3-4pm</p>
                <p>Friend Center 010</p>
              </div>
              <div class="profile-details">
                <p><strong>Raj H. Ghugare</strong></p>
                <p>Office Hours: Mon 3-4pm</p>
                <p>COS Building 003</p>
              </div>
              <div class="profile-details">
                <p><strong>Chongyi Zheng</strong></p>
                <p>Office Hours: Thu 3-4pm</p>
                <p>COS Building 302</p>
              </div>
            </div>
          </div>
      </div>



      <section id="course-description">
        <h3>Course Description</h3>
        <p>This course provides an introductory overview of reinforcement learning (RL), a machine learning paradigm where agents learn to make decisions by interacting with their environment. We will cover fundamental concepts such as Markov Decision Processes, value functions, and policy optimization. Students will learn important RL algorithms including Q-learning, policy gradient methods, and actor-critic approaches. We will also address key challenges in RL such as exploration, generalization, and sample efficiency. Applications of RL to real-world problems—including robotics, healthcare, and molecular science—will be highlighted throughout the course. Assignments will involve implementing RL algorithms and conducting mathematical analyses. Students will complete an open-ended final group project.</p>
        
        <h4 style="margin-top: 1.5em;">Prerequisites</h4>
        <p>Students should have a solid foundation in machine learning and mathematics, including familiarity with probability, statistics, and linear algebra. Prior completion of courses such as COS 324 (Introduction to Machine Learning) or equivalent is recommended. Programming experience in Python is required.</p>
      </section>

      <section id="grading">
        <h3>Course Expectations & Grading</h3>
        <div class="grading-info">
          <h4>Components</h4>
          <ul style="list-style: none; padding-left: 0;">
            <li style="margin-bottom: 1em"><strong>Participation (15%):</strong> Starting week 3: Google form with in-class polling questions; breakout discussions on assigned papers; submit reading reflections on assigned papers with the marked up PDF of the paper.</li>
            
            <li style="margin-bottom: 1em"><strong>Problem Sets (15%):</strong> 3 assignments, due every other week starting on week 3; small theory problems.</li>

            <li style="margin-bottom: 1em"><strong>Programming Assignments (20%):</strong> 3 assignments, starting on week 3; small programming tasks.</li>
            
            <li style="margin-bottom: 1em"><strong>Final Project (50%):</strong> The biggest component! Research project on a topic in RL; aim for academic workshop-level quality.</li>
          </ul>
          
          <h4 style="margin-top: 1.5em;">Policies</h4>
          <ul style="list-style: disc; padding-left: 1.5em;">
            <li><strong>Late Submissions:</strong> Late assignments will incur a penalty of 10% per day, up to a maximum of three days. After three days, assignments will not be accepted unless prior arrangements are made.</li>
            <li><strong>Academic Integrity:</strong> Students are expected to adhere to Princeton University's academic integrity policies. Using LLMs for solving assignments is NOT permitted other than for getting basic understanding and learning, you must understand and be able to explain all code you submit. That being said, we are okay with some small amount of LLM usage for understanding concepts and ideas, as well as helping with code for more complicated projects—but only minimimally for writing as a post-draft check! But again, you are responsible for the content.</li>
            <li><strong>Collaboration:</strong> You may discuss problem sets with classmates and work together with up to 5 people. List collaborators on your submission and write-up/submit solutions individually.</li>
          </ul>
        </div>
      </section>

      <section id="resources">
        <h3>Resources</h3>
        <div class="grading-info">
          <h4>Lecture Notes</h4>
          <ul style="list-style: disc; padding-left: 1.5em;">
            <li><a href="main.pdf" target="_blank">Lecture notes (PDF)</a></li>
          </ul>
          
          <h4>Textbook</h4>
          <ul style="list-style: disc; padding-left: 1.5em;">
            <li><strong>Required:</strong> None — lecture notes are posted on the course website (see above).</li>
          </ul>
          
          <h4 style="margin-top: 1em;">Optional Textbooks</h4>
          <ul style="list-style: disc; padding-left: 1.5em;">
            <li><em>Reinforcement Learning: An Introduction</em> by Richard S. Sutton and Andrew G. Barto</li>
            <li><em>Reinforcement Learning: Bit by Bit</em> by Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and Zheng Wen</li>
            <li><em>Bandit Algorithms</em> by Tor Lattimore and Csaba Szepesvári (if you're interested in bandits)</li>
            <li><em>Algorithms for Reinforcement Learning</em> by Csaba Szepesvári</li>
            <li><em>Mathematical Foundations of Reinforcement Learning</em> by Shiyu Zhao</li>
            <li><em>An Introduction to Deep Reinforcement Learning</em> by V. François-Lavet, P. Henderson, R. Islam, M.G. Bellemare, and J. Pineau</li>
            <li><em>Markov Decision Processes: Discrete Stochastic Dynamic Programming</em> by Martin L. Puterman</li>
            <li><em>Theoretical Neuroscience</em> by Peter Dayan and Laurence F. Abbott (Chapters 8–10)</li>
          </ul>
          
          <h4 style="margin-top: 1em;">Supplementary Materials</h4>
          <ul style="list-style: disc; padding-left: 1.5em;">
            <li>Selected research papers for advanced topics</li>
            <li>OpenAI Spinning Up in Deep RL <a href="https://spinningup.openai.com/" target="_blank">[Link]</a></li>
          </ul>
        </div>
      </section>

      <section id="assignments">
        <h3>Assignments</h3>
        <div class="grading-info">
          <h4>Problem Set and Coding Assignment 1</h4>
          We have provided both the assignment and the TeX file for use as a template. Unless explicitly specified, please type up your solutions using TeX and submit your compiled PDF, generated plots, and completed hw.py file on Gradescope. 
          <p>
            <a href="hw/hw1/hw1.pdf" target="_blank">PDF</a> | <a href="hw/hw1/hw1.tex" target="_blank">.TeX</a> | <a href="hw/hw1/hw1.zip" target="_blank">code.zip</a>
          </p>
          <p style="margin-top: 0.5em; font-style: italic; color: #555;">We will post submission instructions on Canvas.</p>
        </div>
      </section>

      <section id="project">
        <h3>Final Project</h3>
        <div class="grading-info">
          <p>The final project is the largest component of the course (50%). You will work in groups of 3–5 to complete a research project on a topic in reinforcement learning, aiming for academic workshop-level quality.</p>
          <p style="margin-top: 1em;">
            <a href="https://docs.google.com/document/d/1xZ1KY_qEcM3qPa8KqybQyXGPxqVkrPzRS053zN0NSxQ/edit?usp=sharing" target="_blank" style="display: inline-block; padding: 0.6em 1.2em; background: #EE7F2D; color: white; border-radius: 6px; text-decoration: none; font-weight: 600;">View Project Instructions &rarr;</a>
          </p>
        </div>
      </section>

      <section id="schedule">
        <h3>Course Schedule</h3>
        <p style="font-size: 0.95em; font-style: italic; color: #555; margin-bottom: 1.5em;">
          Schedule is tentative and subject to change. Check the course website for the most up-to-date information.
        </p>
        <div class="table-responsive">
          <table class="table table-striped">
            <thead>
              <tr>
                <th style="width: 8%;">WEEK</th>
                <th style="width: 20%;">TOPIC</th>
                <th style="width: 68%;">DESCRIPTION & READINGS</th>
              </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1</td>
                    <td>Course Introduction & Foundations</td>
                    <td>
                      <strong>Lecture 1 (Jan 30):</strong> Course intro, what is RL, the Markov Decision Process (MDP), value iteration, and policy iteration.
                      <br><a href="lecture01_what_is_rl.pdf" target="_blank">[Slides]</a>
                      <br><strong>Optional Textbook Coverage:</strong>
                      <ul>
                        <li>Sutton &amp; Barto: Ch 3 (Finite Markov Decision Processes), Ch 4 (Dynamic Programming)</li>
                        <li>Puterman: Ch 2 (Model Formulation), Ch 6 (Discounted Markov Decision Problems)</li>
                        <li>Szepesvári: Ch 1 (Markov Decision Processes)</li>
                        <li>Zhao: Ch 1–4 (Basic Concepts through Value Iteration and Policy Iteration)</li>
                        <li>François-Lavet et al.: Ch 3 (Introduction to Reinforcement Learning)</li>
                        <li>Dayan &amp; Abbott: Ch 9 (Classical Conditioning and Reinforcement Learning)</li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>Value-based RL</td>
                    <td>
                      <strong>Lecture 2 (Feb 6):</strong> Q-learning, value-based methods, and value function learning.
                      <br><a href="lecture02_value_based_rl.pdf" target="_blank">[Slides (pre-lecture)]</a>
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1312.5602" target="_blank">Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)</a></li>
                        <li><a href="https://arxiv.org/abs/1710.02298" target="_blank">Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)</a></li>
                        <li><a href="https://www.cis.upenn.edu/~mkearns/papers/tdlambda.pdf" target="_blank">“Bias-Variance” Error Bounds
                          for Temporal Difference Updates (Kearns & Singh 2000)</a></li>
                      </ul>
                      <strong>Optional Readings:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/2404.12358" target="_blank">From r to Q*: Your Language Model is Secretly a Q-Function (Rafailov et al., 2024)</a></li>
                        <li><a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf" target="_blank">Q-learning (Watkins & Dayan, 1992)</a></li>
                      </ul>
                      <strong>Optional Textbook Coverage:</strong>
                      <ul>
                        <li>Sutton &amp; Barto: Ch 6 (Temporal-Difference Learning), Ch 9–10 (On-policy Prediction and Control with Approximation)</li>
                        <li>Szepesvári: Ch 2 (Value Prediction Problems)</li>
                        <li>Zhao: Ch 7 (Temporal-Difference Methods), Ch 8 (Value Function Methods)</li>
                        <li>François-Lavet et al.: Ch 4 (Value-Based Methods for Deep RL)</li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>Value-based RL (cont'd)</td>
                    <td>
                      <strong>Lecture 3 (Feb 13):</strong> Continuation of value-based methods and DDPG.
                      <br><a href="lecture03_deep_q_learning.pdf" target="_blank">[Slides]</a>
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1312.5602" target="_blank">Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)</a></li>
                        <li><a href="https://arxiv.org/abs/1710.02298" target="_blank">Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)</a></li>
                        <li><a href="https://www.cis.upenn.edu/~mkearns/papers/tdlambda.pdf" target="_blank">"Bias-Variance" Error Bounds for Temporal Difference Updates (Kearns &amp; Singh 2000)</a></li>
                        <li><a href="https://arxiv.org/abs/1509.02971" target="_blank">Continuous control with deep reinforcement learning [DDPG] (Lillicrap et al., 2015)</a></li>
                      </ul>
                      <strong>Optional Textbook Coverage:</strong>
                      <ul>
                        <li>Sutton &amp; Barto: Ch 6 (Temporal-Difference Learning), Ch 9–10 (On-policy Prediction and Control with Approximation)</li>
                        <li>Szepesvári: Ch 2 (Value Prediction Problems), Ch 3 (Control)</li>
                        <li>Zhao: Ch 7 (Temporal-Difference Methods), Ch 8 (Value Function Methods)</li>
                        <li>François-Lavet et al.: Ch 4 (Value-Based Methods for Deep RL)</li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>Policy Gradient and Actor-Critic Methods</td>
                    <td>
                      <strong>Lecture 4 (Feb 20):</strong> REINFORCE, policy gradients, and TRPO.
                      <br><a href="lecture04_policy_gradients.pdf" target="_blank">[Slides]</a>
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://link.springer.com/article/10.1007/BF00992696" target="_blank">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning [REINFORCE] (Williams, 1992)</a></li>
                        <li><a href="https://proceedings.neurips.cc/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html" target="_blank">A Natural Policy Gradient (Kakade, 2001)</a></li>
                        <li><a href="https://arxiv.org/abs/1502.05477" target="_blank">Trust Region Policy Optimization [TRPO] (Schulman et al., 2015)</a></li>
                        <li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf" target="_blank">Approximately Optimal Approximate Reinforcement Learning (Kakade &amp; Langford, 2002)</a></li>
                        <li><a href="https://arxiv.org/abs/1908.00261" target="_blank">On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift (Agarwal et al., 2019)</a></li>
                      </ul>
                      <strong>Optional Textbook Coverage:</strong>
                      <ul>
                        <li>Sutton &amp; Barto: Ch 13 (Policy Gradient Methods)</li>
                        <li>Szepesvári: Ch 3 (Control — actor-critic methods)</li>
                        <li>Zhao: Ch 9 (Policy Gradient Methods)</li>
                        <li>François-Lavet et al.: Ch 5 (Policy Gradient Methods for Deep RL)</li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>Actor-Critic Methods</td>
                    <td>
                      <strong>Lecture 5 (Feb 27):</strong> Bias-variance trade-offs, actor-critic methods, baselines as control variates, and GRPO.
                      <br><a href="lecture-5-notes.pdf" target="_blank">[Notes]</a>

                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1707.06347" target="_blank">Proximal Policy Optimization Algorithms [PPO] (Schulman et al., 2017)</a></li>
                        <li><a href="https://arxiv.org/abs/2402.03300" target="_blank">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models [GRPO] (Shao et al., 2024)</a></li>
                        <li><a href="https://arxiv.org/abs/1708.05144" target="_blank">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation [ACKTR] (Wu et al., 2017)</a></li>
                        <li><a href="https://www.jmlr.org/papers/volume5/greensmith04a/greensmith04a.pdf" target="_blank">Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning (Greensmith, Bartlett &amp; Baxter, 2004)</a></li>
                        <li><a href="https://proceedings.mlr.press/v139/chung21a/chung21a.pdf" target="_blank">Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization (Chung et al., 2021)</a></li>
                        <li><a href="https://arxiv.org/abs/1802.09477" target="_blank">Addressing Function Approximation Error in Actor-Critic Methods [TD3] (Fujimoto et al., 2018)</a></li>
                      </ul>
                      <strong>Optional Textbook Coverage:</strong>
                      <ul>
                        <li>Sutton &amp; Barto: Ch 13 (Policy Gradient Methods)</li>
                        <li>Szepesvári: Ch 3 (Control — actor-critic methods)</li>
                        <li>Zhao: Ch 10 (Actor-Critic Methods)</li>
                        <li>François-Lavet et al.: Ch 5 (Policy Gradient Methods for Deep RL)</li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>Exploration</td>
                    <td>
                      <strong>Lecture 6 (Mar 6):</strong> Exploration and exploitation. Entropy regularization in policy gradients and convergence properties. Maximum entropy RL framework. Exploration in the bandit setting: UCB, Thompson Sampling, and regret bounds.
                      <br><strong>All optional:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1801.01290" target="_blank">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor [SAC] (Haarnoja et al., 2018)</a></li>
                        <li><a href="https://arxiv.org/abs/2007.06558" target="_blank">Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization (Cen et al., 2021)</a></li>
                        <li><a href="https://arxiv.org/abs/1704.06440" target="_blank">Equivalence Between Policy Gradients and Soft Q-Learning (Schulman, Chen &amp; Abbeel, 2017)</a></li>
                        <li><a href="https://arxiv.org/pdf/1706.01905" target="_blank">Parameter Space Noise for Exploration (Plappert et al., 2018)</a></li>
                        <li><a href="https://proceedings.mlr.press/v80/haarnoja18b.html" target="_blank">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (Haarnoja et al., 2018)</a></li>
                        <li><a href="https://proceedings.mlr.press/v119/mei20b/mei20b.pdf" target="_blank">On the Global Convergence Rates of Softmax Policy Gradient Methods (Mei et al., 2020)</a></li>
                      </ul>
                      <strong>Additional Optional Background:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1805.00909" target="_blank">Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review (Levine, 2018)</a></li>
                        <li>Lattimore &amp; Szepesvári: Ch 6-10 (Stochastic Bandits)
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>7</td>
                    <td>Exploration (cont'd)</td>
                    <td>
                      <strong>Lecture 7 (Mar 13):</strong> Exploration continued — Bayesian and information-theoretic perspectives. Thompson Sampling theory, Information-Directed Sampling, and Posterior Sampling for RL.
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1403.5556" target="_blank">Learning to Optimize via Information-Directed Sampling (Russo &amp; Van Roy, 2018)</a></li>
                        <li><a href="https://arxiv.org/abs/1707.02038" target="_blank">A Tutorial on Thompson Sampling (Russo, Van Roy, Kazerouni, Osband &amp; Wen, 2018)</a></li>
                        <li><a href="https://arxiv.org/abs/1306.0940" target="_blank">(More) Efficient Reinforcement Learning via Posterior Sampling (Osband, Russo &amp; Van Roy, 2013)</a></li>
                        <li><a href="https://arxiv.org/abs/1607.00215" target="_blank">Why is Posterior Sampling Better than Optimism for Reinforcement Learning? (Osband &amp; Van Roy, 2017)</a></li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>Reward Specification &amp; Shaping</td>
                    <td>
                      <strong>Lecture 8 (Mar 27):</strong> Reward specification, misspecification, and reward shaping. When is reward "enough"? Potential-based shaping, reward hacking, and the challenges of designing good reward functions.
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://www.sciencedirect.com/science/article/pii/S0004370221000862" target="_blank">Reward is Enough (Silver, Singh, Precup &amp; Sutton, 2021)</a></li>
                        <li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf" target="_blank">Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping (Ng, Harada &amp; Russell, 1999)</a></li>
                        <li><a href="https://arxiv.org/abs/2209.13085" target="_blank">Defining and Characterizing Reward Hacking (Skalse, Howe, Krasheninnikov &amp; Krueger, 2022)</a></li>
                        <li><a href="https://arxiv.org/abs/2201.03544" target="_blank">The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models (Pan, Bhatia &amp; Steinhardt, 2022)</a></li>
                      </ul>
                    </td>
                </tr>
                <tr>
                    <td>9</td>
                    <td>RLHF &amp; Alignment</td>
                    <td>
                      <strong>Lecture 9 (Apr 3):</strong> Reinforcement learning from human feedback (RLHF), reward modeling from preferences, Direct Preference Optimization, and AI alignment.
                      <br><strong>Pick any two:</strong>
                      <ul>
                        <li><a href="https://arxiv.org/abs/1706.03741" target="_blank">Deep Reinforcement Learning from Human Preferences (Christiano et al., 2017)</a></li>
                        <li><a href="https://arxiv.org/abs/2203.02155" target="_blank">Training Language Models to Follow Instructions with Human Feedback [InstructGPT] (Ouyang et al., 2022)</a></li>
                        <li><a href="https://arxiv.org/abs/2305.18290" target="_blank">Direct Preference Optimization: Your Language Model is Secretly a Reward Model [DPO] (Rafailov et al., 2023)</a></li>
                        <li><a href="https://arxiv.org/abs/2009.01325" target="_blank">Learning to Summarize from Human Feedback (Stiennon et al., 2020)</a></li>
                        <li><a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)</a></li>
                      </ul>
                    </td>
                </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section id="faq">
        <h3>Frequently Asked Questions</h3>
        <div class="grading-info">
          <h4>How do I enroll in this course?</h4>
          <p>This course is closed for enrollment.</p>
          <h4 style="margin-top: 1.5em;">What are the prerequisites?</h4>
          <p>Students should have completed COS 324 (Introduction to Machine Learning) or an equivalent course. Familiarity with probability, statistics, linear algebra, and Python programming is required.</p>
          
          <h4 style="margin-top: 1.5em;">Is this course suitable for graduate students?</h4>
          <p>Yes! This course is open to both undergraduate and graduate students. Graduate students may be expected to complete a more advanced final project.</p>
          
          <h4 style="margin-top: 1.5em;">What programming language will we use?</h4>
          <p>All assignments will be in Python using standard ML libraries (NumPy, PyTorch). Familiarity with these tools is helpful but not required—we will provide tutorials.</p>
          
          <h4 style="margin-top: 1.5em;">Can the final project be individual?</h4>
          <p>Generally no. Due to the size of the enrollment, we will require 3-5 students per group, except in exceptional circumstances.</p>

          <h4 style="margin-top: 1.5em;">Can I audit the course?</h4>
          <p>Formal auditing is not possible, but if there's room you can sit in on lectures.</p>
        </div>
      </section>


      <style>
        body {
          font-family: system-ui, -apple-system, sans-serif;
          line-height: 1.5;
          color: #333;
          padding-bottom: 3em;
        }
        
        .container {
          max-width: 1200px;
          margin: 0 auto;
          padding: 2em;
        }
        
        .header-row {
          display: flex;
          justify-content: space-between;
          align-items: start;
          margin: 2em 0;
          flex-wrap: wrap;
          gap: 1em;
        }
        
        .namecard-container {
          display: flex;
          gap: 2em;
          margin: 2em 0;
          flex-wrap: wrap;
        }
        
        .namecard {
          flex: 1;
          min-width: 300px;
          border: 1px solid #ddd;
          border-radius: 8px;
          overflow: hidden;
        }
        
        .namecard-header {
          background: #EE7F2D;
          color: white;
          margin: 0;
          padding: 1em;
          font-weight: 600;
        }
        
        .namecard-content {
          padding: 1em;
        }
        
        .profile-details p {
          margin: 0.5em 0;
        }
        
        section {
          margin: 3em 0;
        }
        
        h3 {
          color: #EE7F2D;
          margin-bottom: 1em;
        }
        
        h4 {
          color: #D46B1A;
        }
        
        .table-responsive {
          overflow-x: auto;
        }
        
        .table {
          margin-top: 1em;
        }
        
        .table th {
          background: #f8f9fa;
          padding: 1em;
        }
        
        .table td {
          vertical-align: top;
          padding: 1.2em 0.8em;
        }
        
        .table ul {
          padding-left: 1.2em;
          margin-bottom: 0.5em;
        }
        
        .table p {
          margin-bottom: 0.8em;
        }
        
        td strong {
          display: block;
          margin-top: 0.8em;
          color: #EE7F2D;
        }
        
        td a {
          color: #2E5799;
          text-decoration: none;
        }
        
        td a:hover {
          text-decoration: underline;
        }
        
        .grading-info {
          background: #f8f9fa;
          padding: 2em;
          border-radius: 8px;
        }
        
        .important-notes {
          margin-top: 2em;
          padding-top: 2em;
          border-top: 1px solid #ddd;
        }
        
        @media (max-width: 768px) {
          .container {
            padding: 1em;
          }

          h1 {
            font-size: 1.4rem;
          }

          .header-row {
            flex-direction: column;
          }

          .nav-links {
            display: flex;
            flex-wrap: wrap;
            gap: 0.3em;
            font-size: 0.9em;
          }

          .namecard {
            flex: 100%;
            min-width: unset;
          }

          .namecard-content[style*="grid-template-columns"] {
            grid-template-columns: 1fr !important;
          }

          .table th,
          .table td {
            padding: 0.6em 0.4em;
            font-size: 0.9em;
          }

          .grading-info {
            padding: 1em;
          }
        }
      </style>
    </div>
  </body>
        <!--  copyright notice and footer-->
    <footer>
      
      <div class="container">
        <p style="text-align: center; font-size: 0.8em; color: #666;">&copy; Peter Henderson. All rights reserved. Style inspired by <a href="https://joonspk-research.github.io/cs222-fall24/">Stanford CS 222 Webpage.</a></p>
      </div>          
</html>

